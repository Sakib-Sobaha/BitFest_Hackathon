{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import (\n    BartForConditionalGeneration,\n    BartTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq,\n    TrainerCallback\n)\nfrom sklearn.model_selection import train_test_split\nimport os\nimport numpy as np\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T15:25:19.085128Z","iopub.execute_input":"2024-12-21T15:25:19.085419Z","iopub.status.idle":"2024-12-21T15:25:19.089624Z","shell.execute_reply.started":"2024-12-21T15:25:19.085396Z","shell.execute_reply":"2024-12-21T15:25:19.088781Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\ndef normalize_text(text):\n    \"\"\"Normalize Banglish text for consistency.\"\"\"\n    # Example normalization rules\n    text = text.lower()\n    # text = text.replace(\"ki\", \"‡¶ï‡¶ø\").replace(\"tumi\", \"‡¶§‡ßÅ‡¶Æ‡¶ø\")  # Add more mappings\n    return text\n\n\n# Custom callback to track epoch-wise losses\nclass LossCallback(TrainerCallback):\n    def __init__(self):\n        self.training_losses = []\n        self.eval_losses = []\n        self.current_epoch_losses = []\n        \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is not None:\n            # print(logs)\n            if 'train_loss' in logs:\n                self.current_epoch_losses.append(logs['train_loss'])\n            if 'eval_loss' in logs:\n                self.eval_losses.append(logs['eval_loss'])\n                \n    def on_epoch_end(self, args, state, control, **kwargs):\n        if self.current_epoch_losses:\n            avg_loss = np.mean(self.current_epoch_losses)\n            self.training_losses.append(avg_loss)\n            print(f\"\\nEpoch {state.epoch}: Average Training Loss = {avg_loss:.4f}\")\n            if self.eval_losses:\n                print(f\"Epoch {state.epoch}: Validation Loss = {self.eval_losses[-1]:.4f}\")\n            self.current_epoch_losses = []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T15:25:19.106074Z","iopub.execute_input":"2024-12-21T15:25:19.106294Z","iopub.status.idle":"2024-12-21T15:25:19.112154Z","shell.execute_reply.started":"2024-12-21T15:25:19.106266Z","shell.execute_reply":"2024-12-21T15:25:19.111172Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\n# Clear GPU cache\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load dataset\nds = load_dataset(\"SKNahin/bengali-transliteration-data\")\ntrain_val_data = ds['train']\n\n# Take a smaller subset for initial testing\nMAX_SAMPLES = 10000  # Adjust this number based on your GPU memory\nbanglish_texts = train_val_data['rm'][:MAX_SAMPLES]\nbangla_texts = train_val_data['bn'][:MAX_SAMPLES]\n\nbanglish_texts = [normalize_text(text) for text in banglish_texts]\n\n\n# Split data\ntrain_banglish, val_banglish, train_bangla, val_bangla = train_test_split(\n    banglish_texts, bangla_texts, test_size=0.1, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T15:25:19.126509Z","iopub.execute_input":"2024-12-21T15:25:19.126760Z","iopub.status.idle":"2024-12-21T15:25:19.696197Z","shell.execute_reply.started":"2024-12-21T15:25:19.126740Z","shell.execute_reply":"2024-12-21T15:25:19.695516Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"\n# Initialize tokenizer and model\nmodel_name = \"facebook/bart-base\"\ntokenizer = BartTokenizer.from_pretrained(model_name)\n\ndef preprocess_data(banglish_texts, bangla_texts):\n    inputs = tokenizer(\n        banglish_texts, \n        padding=True,\n        truncation=True,\n        max_length=64,\n        return_tensors=\"pt\"\n    )\n    \n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            bangla_texts,\n            padding=True,\n            truncation=True,\n            max_length=64,\n            return_tensors=\"pt\"\n        )\n\n    return {\n        \"input_ids\": inputs[\"input_ids\"],\n        \"attention_mask\": inputs[\"attention_mask\"],\n        \"labels\": labels[\"input_ids\"]\n    }\n\n# Create train and validation datasets\ntrain_encodings = preprocess_data(train_banglish, train_bangla)\nval_encodings = preprocess_data(val_banglish, val_bangla)\n\nclass SimpleDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: val[idx] for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n# Create datasets\ntrain_dataset = SimpleDataset(train_encodings)\nval_dataset = SimpleDataset(val_encodings)\n\n# Initialize model\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(device)\n\n# Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./simple-banglish-translator\",\n    evaluation_strategy=\"epoch\",  # Changed to epoch to get per-epoch evaluation\n    learning_rate=0.01,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    gradient_accumulation_steps=32,\n    num_train_epochs=15,\n    save_steps=100,\n    save_total_limit=2,\n    fp16=True,\n    logging_steps=50,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    report_to=\"none\",\n    save_strategy=\"epoch\"  # Changed to epoch\n)\n\n# Initialize loss callback\nloss_callback = LossCallback()\n\n# Initialize trainer with callback\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n    callbacks=[loss_callback]  # Add the callback\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T15:25:19.697151Z","iopub.execute_input":"2024-12-21T15:25:19.697399Z","iopub.status.idle":"2024-12-21T15:25:22.469210Z","shell.execute_reply.started":"2024-12-21T15:25:19.697362Z","shell.execute_reply":"2024-12-21T15:25:22.468248Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"\n# Training\nprint(\"Starting training...\")\ntrainer.train()\n\n# Print final loss summary\nprint(\"\\nTraining Summary:\")\nfor epoch, (train_loss, eval_loss) in enumerate(zip(loss_callback.training_losses, loss_callback.eval_losses), 1):\n    print(f\"Epoch {epoch}:\")\n    print(f\"  Average Training Loss: {train_loss:.4f}\")\n    print(f\"  Validation Loss: {eval_loss:.4f}\")\n\n# Save the model\nmodel.save_pretrained(\"./simple-banglish-translator-final\")\ntokenizer.save_pretrained(\"./simple-banglish-translator-final\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T15:25:22.470504Z","iopub.execute_input":"2024-12-21T15:25:22.470749Z","iopub.status.idle":"2024-12-21T15:35:55.560512Z","shell.execute_reply.started":"2024-12-21T15:25:22.470701Z","shell.execute_reply":"2024-12-21T15:35:55.559546Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 10:21, Epoch 13/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>28.049725</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>12.393654</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>15.387169</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>13.630450</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>8.822666</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>8.025673</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>6.051314</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>5.253895</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>No log</td>\n      <td>4.699875</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>No log</td>\n      <td>3.940466</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>No log</td>\n      <td>3.675107</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>9.939200</td>\n      <td>3.403521</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>9.939200</td>\n      <td>3.215353</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>9.939200</td>\n      <td>3.161012</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"\nTraining Summary:\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"('./simple-banglish-translator-final/tokenizer_config.json',\n './simple-banglish-translator-final/special_tokens_map.json',\n './simple-banglish-translator-final/vocab.json',\n './simple-banglish-translator-final/merges.txt',\n './simple-banglish-translator-final/added_tokens.json')"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"def translate_banglish_to_bengali(text):\n    model.eval()\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=64, num_beams=2)\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n\ntest_text = \"ami tomake bhalobashi\"\ntranslated = translate_banglish_to_bengali(test_text)\nprint(f\"\\nTest Translation:\")\nprint(f\"Input: {test_text}\")\nprint(f\"Translation: {translated}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T15:35:55.561609Z","iopub.execute_input":"2024-12-21T15:35:55.561939Z","iopub.status.idle":"2024-12-21T15:35:56.588151Z","shell.execute_reply.started":"2024-12-21T15:35:55.561907Z","shell.execute_reply":"2024-12-21T15:35:56.587235Z"}},"outputs":[{"name":"stdout","text":"\nTest Translation:\nInput: ami tomake bhalobashi\nTranslation: ÔøΩÔøΩÔøΩÔøΩ‡¶ßÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩ‡¶ßÔøΩÔøΩÔøΩ ‡¶æÔøΩ‡¶æ‡¶®ÔøΩ‡¶®‡¶áÔøΩ‡¶áÔøΩÔøΩ‡¶ß‡¶ßÔøΩÔøΩ\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import torch\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nfrom datasets import load_dataset\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport nltk\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nfrom sklearn.model_selection import train_test_split\n\n# Download required NLTK data\ntry:\n    nltk.download('punkt')\nexcept:\n    pass\n\n\ndef load_model_and_tokenizer(model_path=\"./simple-banglish-translator-final\"):\n    \"\"\"Load the saved model and tokenizer\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    try:\n        tokenizer = BartTokenizer.from_pretrained(model_path)\n        model = BartForConditionalGeneration.from_pretrained(model_path)\n    except:\n        print(\"Saved model not found, loading base model...\")\n        tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n    \n    model.to(device)\n    return model, tokenizer, device\n\ndef translate_text(text, model, tokenizer, device):\n    \"\"\"Translate a single text from Banglish to Bengali\"\"\"\n    model.eval()\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=64, num_beams=2)\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\ndef calculate_character_accuracy(pred, target):\n    \"\"\"Calculate character-level accuracy\"\"\"\n    correct = sum(1 for p, t in zip(pred, target) if p == t)\n    total = max(len(pred), len(target))\n    return correct / total if total > 0 else 0\n\ndef evaluate_model(model, tokenizer, device, test_banglish, test_bangla, num_samples=None):\n    \"\"\"Evaluate the model on test data\"\"\"\n    bleu_scores = []\n    char_accuracies = []\n    predictions = []\n    references = []\n    \n    # Initialize BLEU smoothing function\n    smoothie = SmoothingFunction().method1\n    \n    # Process only num_samples if specified\n    test_range = range(min(len(test_banglish), num_samples if num_samples else len(test_banglish)))\n    \n    for i in tqdm(test_range, desc=\"Evaluating\"):\n        banglish_text = test_banglish[i]\n        bengali_text = test_bangla[i]\n        \n        # Get model prediction\n        pred_bengali = translate_text(banglish_text, model, tokenizer, device)\n        \n        # Calculate BLEU score\n        bleu = sentence_bleu(\n            [bengali_text.split()],\n            pred_bengali.split(),\n            smoothing_function=smoothie\n        )\n        \n        # Calculate character accuracy\n        char_acc = calculate_character_accuracy(pred_bengali, bengali_text)\n        \n        bleu_scores.append(bleu)\n        char_accuracies.append(char_acc)\n        predictions.append(pred_bengali)\n        references.append(bengali_text)\n        \n    # Calculate average scores\n    avg_bleu = np.mean(bleu_scores)\n    avg_char_acc = np.mean(char_accuracies)\n    \n    # Store some example translations\n    examples = []\n    for i in range(min(5, len(predictions))):\n        examples.append({\n            'banglish': test_banglish[i],\n            'predicted': predictions[i],\n            'reference': references[i],\n            'bleu': bleu_scores[i],\n            'char_acc': char_accuracies[i]\n        })\n    \n    results = {\n        'average_bleu': float(avg_bleu),\n        'average_char_accuracy': float(avg_char_acc),\n        'num_samples': len(test_range),\n        'example_translations': examples\n    }\n    \n    return results\n\n\nprint(\"Loading dataset...\")\n\n# 1. Load and Split Dataset\nds = load_dataset(\"SKNahin/bengali-transliteration-data\")\ntrain_val_data = ds['train']  # The dataset comes with only a train split\n\n# Convert to list of dictionaries for easier splitting\ndata_dict = train_val_data.to_dict()\nbanglish_texts = data_dict['rm']\nbangla_texts = data_dict['bn']\n\n# Split into train and validation sets (90-10 split)\ntrain_banglish, val_banglish, train_bangla, val_bangla = train_test_split(\n    banglish_texts, bangla_texts, test_size=0.1, random_state=42\n)\n\n# Load model and tokenizer\nprint(\"Loading model...\")\nmodel, tokenizer, device = load_model_and_tokenizer()\n\n# Evaluate\nprint(\"Starting evaluation...\")\nresults = evaluate_model(model, tokenizer, device, val_banglish, val_bangla, num_samples=100)\n\n# Print results\nprint(\"\\nEvaluation Results:\")\nprint(f\"Number of samples evaluated: {results['num_samples']}\")\nprint(f\"Average BLEU score: {results['average_bleu']:.4f}\")\nprint(f\"Average Character Accuracy: {results['average_char_accuracy']:.4f}\")\n\nprint(\"\\nExample Translations:\")\nfor i, example in enumerate(results['example_translations'], 1):\n    print(f\"\\nExample {i}:\")\n    print(f\"Banglish: {example['banglish']}\")\n    print(f\"Predicted: {example['predicted']}\")\n    print(f\"Reference: {example['reference']}\")\n    print(f\"BLEU Score: {example['bleu']:.4f}\")\n    print(f\"Character Accuracy: {example['char_acc']:.4f}\")\n\n# Save results to file\nwith open('evaluation_results.json', 'w', encoding='utf-8') as f:\n    json.dump(results, f, ensure_ascii=False, indent=2)\nprint(\"\\nResults have been saved to 'evaluation_results.json'\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T15:37:36.069747Z","iopub.execute_input":"2024-12-21T15:37:36.070090Z","iopub.status.idle":"2024-12-21T15:38:19.021418Z","shell.execute_reply.started":"2024-12-21T15:37:36.070061Z","shell.execute_reply":"2024-12-21T15:38:19.020571Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nLoading dataset...\nLoading model...\nStarting evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:41<00:00,  2.42it/s]","output_type":"stream"},{"name":"stdout","text":"\nEvaluation Results:\nNumber of samples evaluated: 100\nAverage BLEU score: 0.0000\nAverage Character Accuracy: 0.0153\n\nExample Translations:\n\nExample 1:\nBanglish: hoi na keno\nPredicted: ÔøΩÔøΩÔøΩÔøΩ‡¶ßÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩ‡¶æÔøΩ‡¶®ÔøΩ‡¶áÔøΩÔøΩÔøΩ‡¶ßÔøΩÔøΩÔøΩ ‡¶æ‡¶®‡¶áÔøΩ‡¶ß‡¶ßÔøΩ‡¶ßÔøΩ‡¶ß \nReference: ‡¶π‡ßü ‡¶®‡¶æ ‡¶ï‡ßá‡¶®\nBLEU Score: 0.0000\nCharacter Accuracy: 0.0000\n\nExample 2:\nBanglish: 15k budget a 635 moteo jay na\nPredicted: ÔøΩÔøΩÔøΩÔøΩ‡¶ßÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩ‡¶ßÔøΩÔøΩÔøΩ ‡¶æÔøΩ‡¶æ‡¶®ÔøΩ‡¶®‡¶áÔøΩ‡¶áÔøΩÔøΩ‡¶ß‡¶ßÔøΩÔøΩ\nReference: ‡ßß‡ß´‡¶ï ‡¶¨‡¶æ‡¶ú‡ßá‡¶ü ‡¶è ‡ß¨‡ß©‡ß´ ‡¶Æ‡ßã‡¶ü‡ßá‡¶ì ‡¶Ø‡¶æ‡ßü ‡¶®‡¶æ\nBLEU Score: 0.0000\nCharacter Accuracy: 0.0000\n\nExample 3:\nBanglish: Sorry vai dite vule gesi\nPredicted: ÔøΩÔøΩÔøΩÔøΩ‡¶ß‡¶ßÔøΩ‡¶ßÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩ ‡¶æÔøΩ‡¶æ‡¶®ÔøΩ‡¶®‡¶áÔøΩ‡¶áÔøΩÔøΩÔøΩ‡¶ßÔøΩ‡¶ßÔøΩ‡¶ß\nReference: ‡¶∏‡¶∞‡¶ø ‡¶≠‡¶æ‡¶á ‡¶¶‡¶ø‡¶§‡ßá ‡¶≠‡ßÅ‡¶≤‡ßá ‡¶ó‡ßá‡¶õ‡¶ø\nBLEU Score: 0.0000\nCharacter Accuracy: 0.0286\n\nExample 4:\nBanglish: fast kaj kore ki?\nPredicted: ÔøΩÔøΩÔøΩÔøΩ‡¶ßÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩ‡¶æÔøΩ‡¶®ÔøΩ‡¶áÔøΩÔøΩÔøΩ‡¶ßÔøΩÔøΩÔøΩ ‡¶æ‡¶®‡¶áÔøΩ‡¶ß‡¶ßÔøΩ‡¶ßÔøΩ‡¶ß \nReference: ‡¶´‡¶æ‡¶∏‡ßç‡¶ü ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá ‡¶ï‡¶ø?\nBLEU Score: 0.0000\nCharacter Accuracy: 0.0000\n\nExample 5:\nBanglish: Apnar prime account ta ektu diben \nPredicted: ÔøΩÔøΩÔøΩÔøΩ‡¶ßÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩ‡¶ßÔøΩÔøΩÔøΩ ‡¶æÔøΩ‡¶æ‡¶®ÔøΩ‡¶®‡¶áÔøΩ‡¶áÔøΩÔøΩÔøΩ‡¶ß‡¶ßÔøΩ‡¶ßÔøΩ‡¶ß \nReference: ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶æ‡¶á‡¶Æ ‡¶è‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶ü‡¶æ ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶¶‡¶ø‡¶¨‡ßá‡¶®\nBLEU Score: 0.0000\nCharacter Accuracy: 0.0000\n\nResults have been saved to 'evaluation_results.json'\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":21}]}