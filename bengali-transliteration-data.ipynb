{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T12:16:27.158237Z","iopub.execute_input":"2024-12-21T12:16:27.158864Z","iopub.status.idle":"2024-12-21T12:16:27.163175Z","shell.execute_reply.started":"2024-12-21T12:16:27.158824Z","shell.execute_reply":"2024-12-21T12:16:27.162333Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Load The Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndf = pd.read_parquet(\"hf://datasets/SKNahin/bengali-transliteration-data/data/train-00000-of-00001.parquet\")\n\n# Split the dataset into 80% training and 20% validation\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Inspect the splits\nprint(f\"Training samples: {len(train_df)}\")\nprint(f\"Validation samples: {len(val_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T12:23:18.209898Z","iopub.execute_input":"2024-12-21T12:23:18.210212Z","iopub.status.idle":"2024-12-21T12:23:19.101081Z","shell.execute_reply.started":"2024-12-21T12:23:18.210186Z","shell.execute_reply":"2024-12-21T12:23:19.100394Z"}},"outputs":[{"name":"stdout","text":"Training samples: 4004\nValidation samples: 1002\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50\")\n\n# Filter overly short and long sentences\ndef filter_sentences(df):\n    return df[(df['rm'].str.len() > 5) & (df['bn'].str.len() > 5) & \n              (df['rm'].str.len() <= 256) & (df['bn'].str.len() <= 256)]\n\ntrain_df = filter_sentences(train_df)\nval_df = filter_sentences(val_df)\n\n# Tokenize the dataset\ndef tokenize_data(df):\n    inputs = tokenizer(list(df['rm']), max_length=64, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    targets = tokenizer(list(df['bn']), max_length=64, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    return {\"input_ids\": inputs['input_ids'], \"attention_mask\": inputs['attention_mask'], \"labels\": targets['input_ids']}\n\ntrain_data = tokenize_data(train_df)\nval_data = tokenize_data(val_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T12:25:32.079468Z","iopub.execute_input":"2024-12-21T12:25:32.079941Z","iopub.status.idle":"2024-12-21T12:25:39.931771Z","shell.execute_reply.started":"2024-12-21T12:25:32.079917Z","shell.execute_reply":"2024-12-21T12:25:39.931060Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/531 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe2143cc25a347c98426f38d5c77debb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"094ee3dba42b4bb09fa72bc7bea26be7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5bcf627d7744d00ba0fb6fa0a6dd3e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"701ed6395ef64601b6f0207d941ba382"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Model Mbart","metadata":{}},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration\n\n# Load mBART model\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T12:26:41.237096Z","iopub.execute_input":"2024-12-21T12:26:41.237652Z","iopub.status.idle":"2024-12-21T12:26:54.161700Z","shell.execute_reply.started":"2024-12-21T12:26:41.237622Z","shell.execute_reply":"2024-12-21T12:26:54.160780Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1774d1be49c54d2bbb5fa14e76f890e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17046707a3244e0cab6878157e6ad642"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"pip install wandb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T13:05:59.687621Z","iopub.execute_input":"2024-12-21T13:05:59.687917Z","iopub.status.idle":"2024-12-21T13:06:04.037652Z","shell.execute_reply.started":"2024-12-21T13:05:59.687895Z","shell.execute_reply":"2024-12-21T13:06:04.036704Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.9.2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"86d27f6971f8d9777dab3fbc093dc67fd8e3a515\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T13:07:38.666047Z","iopub.execute_input":"2024-12-21T13:07:38.666362Z","iopub.status.idle":"2024-12-21T13:07:38.766260Z","shell.execute_reply.started":"2024-12-21T13:07:38.666331Z","shell.execute_reply":"2024-12-21T13:07:38.765467Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msakibmohammedsobaha\u001b[0m (\u001b[33msakibmohammedsobaha-bangladesh-university-of-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"pip install evaluate\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install sacrebleu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Using mbart","metadata":{}},{"cell_type":"code","source":"# from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n# import torch\n# import evaluate\n\n# # Convert data to PyTorch Dataset\n# class BanglishToBanglaDataset(torch.utils.data.Dataset):\n#     def __init__(self, data):\n#         self.input_ids = data[\"input_ids\"]\n#         self.attention_mask = data[\"attention_mask\"]\n#         self.labels = data[\"labels\"]\n\n#     def __len__(self):\n#         return len(self.input_ids)\n\n#     def __getitem__(self, idx):\n#         return {\n#             \"input_ids\": self.input_ids[idx],\n#             \"attention_mask\": self.attention_mask[idx],\n#             \"labels\": self.labels[idx],\n#         }\n\n\n# # Load BLEU metric\n# bleu = evaluate.load(\"sacrebleu\")\n\n# # Evaluate on validation set\n# def compute_metrics(eval_pred):\n#     predictions, labels = eval_pred\n#     # Decode predictions and labels\n#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n#     # Format labels to match BLEU input requirements\n#     formatted_labels = [[label] for label in decoded_labels]\n#     # Compute BLEU score\n#     bleu_score = bleu.compute(predictions=decoded_preds, references=formatted_labels)\n#     return {\"bleu\": bleu_score[\"score\"]}\n\n# train_dataset = BanglishToBanglaDataset(train_data)\n# val_dataset = BanglishToBanglaDataset(val_data)\n\n# # Define training arguments\n# training_args = Seq2SeqTrainingArguments(\n#     output_dir=\"./results\",\n#     evaluation_strategy=\"epoch\",\n#     learning_rate=5e-5,\n#     per_device_train_batch_size=8,\n#     per_device_eval_batch_size=8,\n#     num_train_epochs=3,\n#     save_strategy=\"epoch\",\n#     logging_dir=\"./logs\",\n#     predict_with_generate=True,\n#     load_best_model_at_end=True,\n# )\n\n# # Initialize Trainer\n# trainer = Seq2SeqTrainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=val_dataset,\n#     tokenizer=tokenizer,\n#     compute_metrics=compute_metrics,  # Include the metric computation\n# )\n\n# # Train the model\n# trainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"\n\n\n\n# trainer.evaluate()\n\n# model.save_pretrained(\"./fine_tuned_mbart\")\n# tokenizer.save_pretrained(\"./fine_tuned_mbart\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install datasets\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip show transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T15:22:48.823871Z","iopub.execute_input":"2024-12-21T15:22:48.824327Z","iopub.status.idle":"2024-12-21T15:22:51.550282Z","shell.execute_reply.started":"2024-12-21T15:22:48.824269Z","shell.execute_reply":"2024-12-21T15:22:51.549384Z"}},"outputs":[{"name":"stdout","text":"Name: transformers\nVersion: 4.44.2\nSummary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\nHome-page: https://github.com/huggingface/transformers\nAuthor: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\nAuthor-email: transformers@huggingface.co\nLicense: Apache 2.0 License\nLocation: /usr/local/lib/python3.10/dist-packages\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\nRequired-by: kaggle-environments\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Train Using ByT5","metadata":{}},{"cell_type":"code","source":"from transformers import ByT5Tokenizer, ByT5ForConditionalGeneration, Trainer, TrainingArguments\nimport evaluate\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\n# Split into train and validation sets (80/20 split)\ntrain_texts_t5, val_texts_t5, train_labels_t5, val_labels_t5 = train_test_split(\n    df['rm'], df['bn'], test_size=0.2, random_state=42\n)\n\n# Create Hugging Face Dataset objects for ByT5\ntrain_dataset_t5 = Dataset.from_dict({\"input_text\": train_texts_t5, \"target_text\": train_labels_t5})\nval_dataset_t5 = Dataset.from_dict({\"input_text\": val_texts_t5, \"target_text\": val_labels_t5})\n\n# Load pre-trained tokenizer and model for ByT5\nmodel_name_t5 = \"google/byt5-small\"  # Use \"google/byt5-small\" for ByT5\ntokenizer_t5 = ByT5Tokenizer.from_pretrained(model_name_t5)\nmodel_t5 = ByT5ForConditionalGeneration.from_pretrained(model_name_t5)\n\n# Preprocessing function for tokenization\ndef preprocess_function_t5(examples):\n    inputs = [f\"transliterate: {text}\" for text in examples[\"input_text\"]]\n    targets = examples[\"target_text\"]\n    model_inputs = tokenizer_t5(inputs, max_length=128, truncation=True)\n    labels = tokenizer_t5(targets, max_length=128, truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Tokenize the datasets for ByT5\ntrain_dataset_t5 = train_dataset_t5.map(preprocess_function_t5, batched=True)\nval_dataset_t5 = val_dataset_t5.map(preprocess_function_t5, batched=True)\n\n# Define training arguments for ByT5\ntraining_args_t5 = TrainingArguments(\n    output_dir=\"./results_byt5\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs_byt5\",\n    logging_steps=100,\n    report_to=\"none\",  # Disable W&B logging if needed\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n)\n\n# Load BLEU metric\nbleu = evaluate.load(\"sacrebleu\")\n\n# Compute BLEU score during evaluation\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Decode the predictions and labels to text\n    decoded_preds = tokenizer_t5.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer_t5.batch_decode(labels, skip_special_tokens=True)\n    \n    # Calculate BLEU score\n    bleu_score = bleu.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n    \n    # Calculate loss (using the `eval_loss` returned from Trainer)\n    eval_loss = eval_pred.metrics[\"eval_loss\"]\n    \n    return {\n        \"eval_loss\": eval_loss,\n        \"bleu\": bleu_score[\"score\"],\n    }\n\n# Initialize Trainer for ByT5 with compute_metrics\ntrainer_t5 = Trainer(\n    model=model_t5,\n    args=training_args_t5,\n    train_dataset=train_dataset_t5,\n    eval_dataset=val_dataset_t5,\n    tokenizer=tokenizer_t5,\n    compute_metrics=compute_metrics  # Add this line to compute BLEU score and loss\n)\n\n# Train the model for ByT5\ntrainer_t5.train()\n\n# Save the model for ByT5\nmodel_t5.save_pretrained(\"./banglish_to_bangla_model_byt5\")\ntokenizer_t5.save_pretrained(\"./banglish_to_bangla_model_byt5\")\n\n# You can also manually evaluate the model after training\nresults_t5 = trainer_t5.evaluate()\n\n# Print evaluation results\nprint(f\"ByT5 Evaluation results: {results_t5}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train using T5","metadata":{}},{"cell_type":"code","source":"# from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n# import evaluate\n\n# # Split into train and validation sets (80/20 split)\n# train_texts_t5, val_texts_t5, train_labels_t5, val_labels_t5 = train_test_split(\n#     df['rm'], df['bn'], test_size=0.2, random_state=42\n# )\n\n# # Create Hugging Face Dataset objects for T5/mT5\n# train_dataset_t5 = Dataset.from_dict({\"input_text\": train_texts_t5, \"target_text\": train_labels_t5})\n# val_dataset_t5 = Dataset.from_dict({\"input_text\": val_texts_t5, \"target_text\": val_labels_t5})\n\n# # Load pre-trained tokenizer and model for T5/mT5\n# model_name_t5 = \"google/mt5-small\"  # Use \"t5-small\" for T5 or \"google/mt5-small\" for mT5\n# tokenizer_t5 = T5Tokenizer.from_pretrained(model_name_t5)\n# model_t5 = T5ForConditionalGeneration.from_pretrained(model_name_t5)\n\n# # Preprocessing function for tokenization\n# def preprocess_function_t5(examples):\n#     inputs = [f\"transliterate: {text}\" for text in examples[\"input_text\"]]\n#     targets = examples[\"target_text\"]\n#     model_inputs = tokenizer_t5(inputs, max_length=128, truncation=True)\n#     labels = tokenizer_t5(targets, max_length=128, truncation=True)\n\n#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n#     return model_inputs\n\n# # Tokenize the datasets for T5/mT5\n# train_dataset_t5 = train_dataset_t5.map(preprocess_function_t5, batched=True)\n# val_dataset_t5 = val_dataset_t5.map(preprocess_function_t5, batched=True)\n\n# # Define training arguments for T5/mT5\n# training_args_t5 = TrainingArguments(\n#     output_dir=\"./results_t5\",\n#     evaluation_strategy=\"epoch\",\n#     learning_rate=5e-5,\n#     per_device_train_batch_size=8,\n#     per_device_eval_batch_size=8,\n#     num_train_epochs=3,\n#     weight_decay=0.01,\n#     save_strategy=\"epoch\",\n#     logging_dir=\"./logs_t5\",\n#     logging_steps=100,\n#     report_to=\"none\",  # Disable W&B logging if needed\n#     load_best_model_at_end=True,\n#     metric_for_best_model=\"eval_loss\",\n# )\n\n# # Load BLEU metric\n# bleu = evaluate.load(\"sacrebleu\")\n\n# # Compute BLEU score during evaluation\n# def compute_metrics(eval_pred):\n#     predictions, labels = eval_pred\n#     # Decode the predictions and labels to text\n#     decoded_preds = tokenizer_t5.batch_decode(predictions, skip_special_tokens=True)\n#     decoded_labels = tokenizer_t5.batch_decode(labels, skip_special_tokens=True)\n    \n#     # Calculate BLEU score\n#     bleu_score = bleu_metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n    \n#     # Calculate loss (using the `eval_loss` returned from Trainer)\n#     eval_loss = eval_pred.metrics[\"eval_loss\"]\n    \n#     return {\n#         \"eval_loss\": eval_loss,\n#         \"bleu\": bleu_score[\"score\"],\n#     }\n\n# # Initialize Trainer for T5/mT5 with compute_metrics\n# trainer_t5 = Trainer(\n#     model=model_t5,\n#     args=training_args_t5,\n#     train_dataset=train_dataset_t5,\n#     eval_dataset=val_dataset_t5,\n#     tokenizer=tokenizer_t5,\n#     compute_metrics=compute_metrics  # Add this line to compute BLEU score and loss\n# )\n\n# # Train the model for T5/mT5\n# trainer_t5.train()\n\n# # Save the model for T5/mT5\n# model_t5.save_pretrained(\"./banglish_to_bangla_model_t5\")\n# tokenizer_t5.save_pretrained(\"./banglish_to_bangla_model_t5\")\n\n# # You can also manually evaluate the model after training\n# results_t5 = trainer_t5.evaluate()\n\n# # Print evaluation results\n# print(f\"T5/mT5 Evaluation results: {results_t5}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate Metrics","metadata":{}},{"cell_type":"code","source":"\n\n# # Load BLEU metric\n# bleu = evaluate.load(\"sacrebleu\")\n\n# # Compute BLEU score during evaluation\n# def compute_metrics(eval_pred):\n#     predictions, labels = eval_pred\n#     # Decode the predictions and labels to text\n#     decoded_preds = tokenizer_t5.batch_decode(predictions, skip_special_tokens=True)\n#     decoded_labels = tokenizer_t5.batch_decode(labels, skip_special_tokens=True)\n    \n#     # Calculate BLEU score\n#     bleu_score = bleu_metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n    \n#     # Calculate loss (using the `eval_loss` returned from Trainer)\n#     eval_loss = eval_pred.metrics[\"eval_loss\"]\n    \n#     return {\n#         \"eval_loss\": eval_loss,\n#         \"bleu\": bleu_score[\"score\"],\n#     }\n\n# # Initialize Trainer for T5/mT5 with compute_metrics\n# trainer_t5 = Trainer(\n#     model=model_t5,\n#     args=training_args_t5,\n#     train_dataset=train_dataset_t5,\n#     eval_dataset=val_dataset_t5,\n#     tokenizer=tokenizer_t5,\n#     compute_metrics=compute_metrics  # Add this line to compute BLEU score and loss\n# )\n\n# # Train the model for T5/mT5\n# trainer_t5.train()\n\n# # Save the model for T5/mT5\n# model_t5.save_pretrained(\"./banglish_to_bangla_model_t5\")\n# tokenizer_t5.save_pretrained(\"./banglish_to_bangla_model_t5\")\n\n# # You can also manually evaluate the model after training\n# results_t5 = trainer_t5.evaluate()\n\n# # Print evaluation results\n# print(f\"T5/mT5 Evaluation results: {results_t5}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}